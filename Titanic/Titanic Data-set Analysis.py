# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1S9DQR-uR0asqH4yewUuM8HjHvtc7F6_P
"""

from google.colab import files
uploaded = files.upload()

import pandas as pd

test = pd.read_csv("test.csv")
train = pd.read_csv("train.csv")

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

test

train

train.info()

train.describe()

train.isnull()

"""# Visualization"""

sns.heatmap(train.isnull(), yticklabels=False, cbar=False, cmap="viridis")

# Number of survivors
sns.set_style("darkgrid")
sns.countplot(x="Survived", data=train)

# Number of survivors by gender
sns.set_style("darkgrid")
sns.countplot(x="Survived", hue="Sex", data=train, palette="rainbow")

# Number of survivors by 'Pclass'
sns.set_style("darkgrid")
sns.countplot(x="Survived", hue="Pclass", data=train, palette="husl")

sns.displot(train["Age"].dropna(), kde=True, color="blue", bins=40)

train["Age"].hist(bins=30, color="darkred",alpha=0.5)

train["Fare"].hist(bins=40, figsize=(12,8))

sns.displot(train["Fare"], kde=True ,color="blue", bins=50)

sns.countplot(x="SibSp", data=train)

import cufflinks as cf
cf.go_offline()

train['Fare'].iplot(kind='hist',bins=30,color='green')

"""# Cleaning
We want to fill in missing age data instead of just dropping the missing age data rows. One way to do this is by filling in the mean age of all the passengers (imputation).
However we can be smarter about this and check the average age by passenger class. For example:

"""

plt.figure(figsize=(12,8))
sns.boxplot(x="Pclass", y="Age", data=train, palette="husl")

"""### We can see that older people are in better classes, which makes sense. So we fill in the missing values according to the average age of that class."""

def fill_age(C):
  Age = C[0]
  Pclass = C[1]

  if pd.isnull(Age):

    if Pclass == 1:
      return 38

    elif Pclass == 2:
      return 29

    else:
      return 24

  else:
    return Age

train["Age"] = train[["Age", "Pclass"]].apply(fill_age, axis=1)

test["Age"] = test[["Age", "Pclass"]].apply(fill_age, axis=1)

train.info()

sns.heatmap(train.isnull(), yticklabels=False,cmap="viridis")

train.drop("Cabin", axis=1, inplace=True)

sns.heatmap(train.isnull(), yticklabels=False, cbar=False, cmap="viridis")

sns.countplot(x="Embarked", data=train)

train["Embarked"].fillna("S", inplace=True)

""".

.

.

# Converting Categorical Features
"""

train.info()

pd.get_dummies(train["Sex"], drop_first=True)

# Use person's title from their names (Mis, Mr, Ms, Master, ...)
train["NameTitle"] = train.Name.apply(lambda x: x.split(",")[1].split(".")[0].strip())

# Created dummy variables from Categories
sex = pd.get_dummies(train["Sex"], drop_first=True)
embark = pd.get_dummies(train["Embarked"], drop_first=True)
name_title = pd.get_dummies(train["Name"], drop_first=True)

# drop Unused Columns
train.drop(["Sex", "Embarked", "Name", "Ticket", "NameTitle"], axis=1, inplace=True)

# Concat Categorical columns and Numeric Columns
train = pd.concat([train, sex, embark, name_title], axis=1)

train.head()

test.head()

test.info()

test["NameTitle"] = test.Name.apply(lambda x: x.split(",")[1].split(".")[0].strip())

sex = pd.get_dummies(test["Sex"], drop_first=True)
embark = pd.get_dummies(test["Embarked"], drop_first=True)
name_title = pd.get_dummies(test["NameTitle"], drop_first=True)


test.drop(["Sex", "Embarked", "Name","Ticket", "Cabin", "NameTitle"], axis=1, inplace=True)

test = pd.concat([test, sex, embark, name_title], axis=1)

test.head()

test["Fare"].fillna((train["Fare"].mean()), inplace=True)

test.info()

""".

.

.

.

# Train Models

### Logistic Regression:
"""

y_train = train["Survived"]
train.drop("Survived", axis=1, inplace=True)

X_train = train
X_train

y_train

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score

LrModel= LogisticRegression(max_iter=2000)
cv = cross_val_score(LrModel, X_train, y_train, cv=5)
LogisticRegression()
print(cv)
print(cv.mean())

from sklearn.model_selection import cross_val_predict

y_pred = cross_val_predict(LrModel, X_train, y_train, cv=5)

from sklearn.metrics import precision_score, recall_score

print(precision_score(y_train, y_pred))
print(recall_score(y_train, y_pred))

from sklearn.metrics import f1_score

f1_score(y_train, y_pred)

from sklearn.metrics import precision_recall_curve

y_scores = cross_val_predict(LrModel, X_train, y_train, cv=5, method="decision_function")

precisions, recalls, thresholds = precision_recall_curve(y_train, y_scores)

def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):
  plt.plot(thresholds, precisions[:-1], "b--", label="Precision")
  plt.plot(thresholds, recalls[:-1], "g-", label="Recall")

plot_precision_recall_vs_threshold(precisions, recalls, thresholds)
plt.show()

"""## Gaussian Model:"""

from sklearn.naive_bayes import GaussianNB

GnbModel = GaussianNB()
cv = cross_val_score(GnbModel, X_train, y_train, cv=5)

print(cv)
print(cv.mean())

"""## Decision Tree Classifier:"""

from sklearn.tree import DecisionTreeClassifier

DtModel = DecisionTreeClassifier(random_state=1)
cv = cross_val_score(DtModel, X_train, y_train, cv=5)

print(cv)
print(cv.mean())

"""## K-Nearest Neighbor:"""

from sklearn.neighbors import KNeighborsClassifier

KnnModel = KNeighborsClassifier(n_neighbors=7)
cv = cross_val_score(KnnModel, X_train, y_train, cv=5)

print(cv)
print(cv.mean())

"""### Random Forest Classifier:"""

from sklearn.ensemble import RandomForestClassifier

Rfc = RandomForestClassifier(n_estimators=200)
cv = cross_val_score(Rfc, X_train, y_train, cv=5)


print(cv)
print(cv.mean())

"""### Support Vector Machine:"""

from sklearn.svm import SVC

SvcModel = SVC(probability=True)
cv = cross_val_score(SvcModel, X_train, y_train, cv=5)


print(cv)
print(cv.mean())

"""### XGB Classifier:"""

from xgboost import XGBClassifier

XgbModel = XGBClassifier(random_state =1,probability=True)
cv = cross_val_score(XgbModel,X_train ,y_train,cv=5)
print(cv)
print(cv.mean())

"""# **Voting**"""

#Voting classifier takes all of the inputs and averages the results. For a "hard" voting classifier each classifier gets 1 vote "yes" or "no" and the result is just a popular vote. For this, you generally want odd numbers
#A "soft" classifier averages the confidence of each of the models. If a the average confidence is > 50% that it is a 1 it will be counted as such
from sklearn.ensemble import VotingClassifier
voting_clf = VotingClassifier(estimators = [('LrModel',LrModel),('GnbModel',GnbModel),('KnnModel',KnnModel),('Rfc',Rfc),('SvcModel',SvcModel),('XgbModel',XgbModel)], voting = 'soft')

cv = cross_val_score(voting_clf,X_train ,y_train,cv=5)
print(cv)
print(cv.mean())

"""# **Predict on Test set**"""

voting_clf.fit(X_train, y_train)

predictions = voting_clf.predict(test).astype(int)
basic_submission = {"PasengerID":test.PassengerID, "Survived":predictions}
base_submission = pd.DataFrame(data=basic_submission)

# Creat .csv file
base_submission.to_csv('base_submission.csv', index=False)